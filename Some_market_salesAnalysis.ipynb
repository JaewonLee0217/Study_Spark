{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KW_MMDS- Colab 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPt5q27L5557"
      },
      "source": [
        "# KW_MMDS - Colab 2\n",
        "## Frequent Pattern Mining in Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsj5WYpR9QId"
      },
      "source": [
        "Let's setup Spark on your Colab environment.  Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-qHai2252mI",
        "outputId": "4325de4e-d3d2-4549-8a71-443c21d65234",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 34kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 41.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=1745563f653d8af2d4fe695d02224ce9aefd69fa65734c9f141a50d64edd2631\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 6 not upgraded.\n",
            "Need to get 35.8 MB of archives.\n",
            "After this operation, 140 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 144617 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u265-b01-0ubuntu2~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u265-b01-0ubuntu2~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CJ71AKe91eh"
      },
      "source": [
        "Now we authenticate a Google Drive client to download the file we will be processing in our Spark job.\n",
        "\n",
        "**Make sure to follow the interactive instructions.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K93ABEy9Zlo"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0orRvrc1-545"
      },
      "source": [
        "id='1dhi1F78ssqR8gE6U-AgB80ZW7V_9snX4'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('products.csv')\n",
        "\n",
        "id='1KZBNEaIyMTcsRV817us6uLZgm-Mii8oU'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('order_products__train.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwtlO4_m_LbQ"
      },
      "source": [
        "If you executed the cells above, you should be able to see the dataset we will need for this Colab under the \"Files\" tab on the left panel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twk-K-jilWK7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr-8fK-1lmY0"
      },
      "source": [
        "Let's initialize the Spark context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOwtm2l7lePt"
      },
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRaF2A_j_nC7"
      },
      "source": [
        "### Your task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebLNUxP0_8x3"
      },
      "source": [
        "If you run successfully the setup stage, you are ready to work with the **3 Million Instacart Orders** dataset. In case you want to read more about it, check the [official Instacart blog post](https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2) about it, a concise [schema description](https://gist.github.com/jeremystan/c3b39d947d9b88b3ccff3147dbcf6c6b) of the dataset, and the [download page](https://www.instacart.com/datasets/grocery-shopping-2017).\n",
        "\n",
        "In this Colab, we will be working only with a small training dataset (~131K orders) to perform fast Frequent Pattern Mining with the FP-Growth algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu-e7Ph2_ruG"
      },
      "source": [
        "products = spark.read.csv('products.csv', header=True, inferSchema=True)\n",
        "orders = spark.read.csv('order_products__train.csv', header=True, inferSchema=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhxZZRT9syUO",
        "outputId": "3d9aa539-b8e5-419b-a3a8-c97877f8e821",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "products.printSchema()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- product_name: string (nullable = true)\n",
            " |-- aisle_id: string (nullable = true)\n",
            " |-- department_id: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VeRYRz2s1pm",
        "outputId": "57d3bcf8-4ed4-4c4e-b1da-86e1a56c3b08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "orders.printSchema()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- add_to_cart_order: integer (nullable = true)\n",
            " |-- reordered: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5muD_Io59CG"
      },
      "source": [
        "Use the Spark Dataframe API to join 'products' and 'orders', so that you will be able to see the product names in each transaction (and not only their ids).  Then, group by the orders by 'order_id' to obtain one row per basket (i.e., set of products purchased together by one customer). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfHoTLAg6qnM"
      },
      "source": [
        "In this Colab we will explore [MLlib](https://spark.apache.org/mllib/), Apache Spark's scalable machine learning library. Specifically, you can use its implementation of the [FP-Growth](https://spark.apache.org/docs/latest/ml-frequent-pattern-mining.html#fp-growth) algorithm to perform efficiently Frequent Pattern Mining in Spark.\n",
        "Use the Python example in the documentation, and train a model with \n",
        "\n",
        "```minSupport=0.01``` and ```minConfidence=0.5```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kpTVdfD8UiO"
      },
      "source": [
        "1. Compute how many frequent itemsets and association rules were generated by running FP-growth.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KYgQ_URunvA",
        "outputId": "76b2ccfe-d4f5-4196-fcb0-adf9c22d9f4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from pyspark.ml.fpm import FPGrowth\n",
        "orders_joined = orders.join(products,orders.product_id == products.product_id )\n",
        "orders_joined = orders_joined.selectExpr([\"order_id\",\"product_name\"])\n",
        "\n",
        "df = orders_joined.groupBy('order_id').agg(collect_set('product_name').alias('items'))\n",
        "\n",
        "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.01, minConfidence=0.5)\n",
        "model = fpGrowth.fit(df)\n",
        "# Display frequent itemsets.\n",
        "print(\"frequent itemsets count: {0}\".format(model.freqItemsets.count()))\n",
        "# Display generated association rules.\n",
        "print(\"association rules count: {0}\".format(model.associationRules.count()))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frequent itemsets : 120\n",
            "association rules : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe-P_Fciz2mi"
      },
      "source": [
        "2. What is the most frequent item?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chK7L-tegqYm",
        "outputId": "0cda26df-07f8-4d4a-aa72-fa51d62bab57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "model.freqItemsets.sort(desc(\"freq\")).show(1)\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-----+\n",
            "|   items| freq|\n",
            "+--------+-----+\n",
            "|[Banana]|18726|\n",
            "+--------+-----+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT8Lwm1VAPoN"
      },
      "source": [
        "3. Now retrain the FP-growth model changing only \n",
        "```minsupport=0.001``` \n",
        "and compute how many frequent itemsets and association rules were generated.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5Fa9spFz_HY",
        "outputId": "2f7de97d-7ab7-454e-97e1-42445957ca10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from pyspark.ml.fpm import FPGrowth\n",
        "orders_joined = orders.join(products,orders.product_id == products.product_id )\n",
        "orders_joined = orders_joined.selectExpr([\"order_id\",\"product_name\"])\n",
        "df = orders_joined.groupBy('order_id').agg(collect_set('product_name').alias('items'))\n",
        "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.001, minConfidence=0.5)\n",
        "UpgradeModel = fpGrowth.fit(df)\n",
        "# Display frequent itemsets.\n",
        "print(\"frequent itemsets : {0}\".format(UpgradeModel.freqItemsets.count()))\n",
        "# Display generated association rules.\n",
        "print(\"association rules : {0}\".format(UpgradeModel.associationRules.count()))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frequent itemsets : 4444\n",
            "association rules : 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kZSknUjei0u"
      },
      "source": [
        "4. Print all the association rules of the above problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UenciqLXgtZD",
        "outputId": "4235811c-6198-4c85-e990-356f4d2a7b0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "UpgradeModel.associationRules.show(truncate=False)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------+------------------------+------------------+------------------+\n",
            "|antecedent                                                       |consequent              |confidence        |lift              |\n",
            "+-----------------------------------------------------------------+------------------------+------------------+------------------+\n",
            "|[Organic Kiwi, Organic Hass Avocado]                             |[Bag of Organic Bananas]|0.5459770114942529|4.627719489738336 |\n",
            "|[Organic Raspberries, Organic Hass Avocado, Organic Strawberries]|[Bag of Organic Bananas]|0.5984251968503937|5.072272070642333 |\n",
            "|[Organic Broccoli, Organic Hass Avocado]                         |[Bag of Organic Bananas]|0.5048231511254019|4.278897986822536 |\n",
            "|[Organic Unsweetened Almond Milk, Organic Hass Avocado]          |[Bag of Organic Bananas]|0.5141065830721003|4.357584667849303 |\n",
            "|[Yellow Onions, Strawberries]                                    |[Banana]                |0.5357142857142857|3.7536332219526702|\n",
            "|[Organic Cucumber, Organic Hass Avocado, Organic Strawberries]   |[Bag of Organic Bananas]|0.546875          |4.635330870478036 |\n",
            "|[Organic Navel Orange, Organic Hass Avocado]                     |[Bag of Organic Bananas]|0.5283018867924528|4.477904539027839 |\n",
            "|[Organic Raspberries, Organic Hass Avocado]                      |[Bag of Organic Bananas]|0.521099116781158 |4.416853618458589 |\n",
            "|[Organic D'Anjou Pears, Organic Hass Avocado]                    |[Bag of Organic Bananas]|0.5170454545454546|4.3824946411792345|\n",
            "|[Organic Navel Orange, Organic Raspberries]                      |[Bag of Organic Bananas]|0.5412186379928315|4.587387356098284 |\n",
            "|[Organic Whole String Cheese, Organic Hass Avocado]              |[Bag of Organic Bananas]|0.5314685314685315|4.504745125675359 |\n",
            "+-----------------------------------------------------------------+------------------------+------------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q77TheWNzMBM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-2tb-BegvbC"
      },
      "source": [
        "5. What can be inferred from the association rules? Write a comment you can get from the rules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qvHiCowh2mc"
      },
      "source": [
        "# 위의 연관 규칙으로 부터 2가지 결론을 도출할 수 있었습니다.\n",
        "#1. 바나나가 가장 잘 팔린다.\n",
        "#2. Organic 제품들을 구매하게 되면 Bag of Organic Bananas을 구매할 가능성이 높다는 결론을 도출할 수 있으며,Organic 품목을 구매한 고객에게는 Bag of Organic Bananas를 추천하거나 물건 배치 시 가까운 곳에 위치시키는 것이 좋다는 데이터 분석 결과를 추론할 수 있습니다.\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}